{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45c84960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfe import OpenFE, transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e633f916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('../')\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from openfe import OpenFE, tree_to_formula, transform\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def get_score(train_x, test_x, train_y, test_y):\n",
    "    train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.2, random_state=2020)\n",
    "    params = {'n_estimators': 1000, 'n_jobs': n_jobs, 'seed': 1}\n",
    "    gbm = lgb.LGBMRegressor(**params)\n",
    "    gbm.fit(train_x, train_y, eval_set=[(val_x, val_y)], callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "    pred = pd.DataFrame(gbm.predict(test_x), index=test_x.index)\n",
    "    score = mean_squared_error(test_y, pred)\n",
    "    return score,test_y, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba674e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"./数据2.0.xlsx\")\n",
    "columns = data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4a7cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = data.corr()\n",
    "# # 选择相关系数的阈值，例如0.1\n",
    "# threshold = 0.2\n",
    "# # 找出低于阈值的特征\n",
    "# low_correlation_features = [index for index in correlation_matrix[\"lattice_distortion\"].index if abs(correlation_matrix[\"lattice_distortion\"][index]) < threshold]\n",
    "# # 删除这些特征\n",
    "# data = data.drop(columns=low_correlation_features)\n",
    "correlation_matrix.to_excel(\"皮尔逊相关系数矩阵-原始特征.xlsx\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424d1e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_excel(\"初步筛选后的特征.xlsx\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c49947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(f_path, seed=88): \n",
    "    df_all = pd.read_excel(f_path)\n",
    "    #create a min max processing object\n",
    "    composition = df_all\n",
    "    scaler = preprocessing.StandardScaler().fit(composition)\n",
    "    normalized_composition = scaler.transform(composition)\n",
    "    return normalized_composition,scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94afa52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trans,scaler = data_generator(\"./数据2.0.xlsx\", seed=42)\n",
    "data = pd.DataFrame(data_trans,columns=columns)\n",
    "data.to_excel(\"归一化后的数据.xlsx\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4fbcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_excel(\"./原始数据2.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29472572",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = data.copy(deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e92b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    n_jobs = 4\n",
    "#     data = fetch_california_housing(as_frame=True).frame\n",
    "#     label = data[['MedHouseVal']]\n",
    "#     del data['MedHouseVal']\n",
    "#     label = -np.log10(1-data[[\"average_coulombic_efficiency\"]]/100)\n",
    "    label=data[[\"Y\"]]\n",
    "    del data[\"Y\"]\n",
    "    train_x, test_x, train_y, test_y = train_test_split(data, label, test_size=0.2, random_state=1)\n",
    "    # get baseline score\n",
    "    score,_,_ = get_score(train_x, test_x, train_y, test_y)\n",
    "    print(\"The MSE before feature generation is\", score)\n",
    "    # feature generation\n",
    "    ofe = OpenFE()\n",
    "    ofe.fit(data=train_x, label=train_y, n_jobs=n_jobs,n_data_blocks = 1)\n",
    "    # OpenFE recommends a list of new features. We include the top 10\n",
    "    # generated features to see how they influence the model performance\n",
    "    train_x, test_x = transform(train_x, test_x, ofe.new_features_list[:10], n_jobs=n_jobs)\n",
    "    score,test_y, pred = get_score(train_x, test_x, train_y, test_y)\n",
    "    print(\"The MSE after feature generation is\", score)\n",
    "    print(\"The top 10 generated features are\")\n",
    "    for feature in ofe.new_features_list[:10]:\n",
    "        print(tree_to_formula(feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d295b04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feature_name = []\n",
    "origin_data = pd.read_excel(\"./数据2.0.xlsx\")\n",
    "for i in list(origin_data.columns)[:-1]:\n",
    "    new_feature_name.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed40d7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ofe.new_features_list[:10]:\n",
    "    new_feature_name.append(tree_to_formula(feature))\n",
    "new_feature_name.append(list(origin_data.columns)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789d782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.concat([train_x,train_y],axis=1)\n",
    "test_data = pd.concat([test_x,test_y],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b270f39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_concat = pd.concat([train_data,test_data],axis=0)\n",
    "data_concat.columns = new_feature_name\n",
    "data_concat.to_excel(\"高价值特征生成.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77cea67",
   "metadata": {},
   "source": [
    "# 贪心算法特征选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b771fb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b763d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f66a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Liner_model(datasets,cols):\n",
    "    skf = KFold(n_splits=5, shuffle=True,random_state=888)\n",
    "    valid_ssr =[]\n",
    "    x = datasets[:,cols]\n",
    "    y = datasets[:,-1:]\n",
    "    for i, (trn_idx, val_idx) in enumerate(skf.split(x, y)):\n",
    "        trn_x, trn_y = x[trn_idx], y[trn_idx]\n",
    "        val_x, val_y = x[val_idx], y[val_idx]\n",
    "        # 创建并拟合线性回归模型\n",
    "        model = LinearRegression()\n",
    "        model.fit(trn_x, trn_y)\n",
    "        # 获得残差和\n",
    "        residual_sum_of_squares = np.sum((model.predict(val_x) - val_y) ** 2)\n",
    "        valid_ssr.append(residual_sum_of_squares)\n",
    "    #返回当前特征组合的交叉验证的残差和\n",
    "    return np.array(valid_ssr).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d7edf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bic(data, features):\n",
    "    # 估计线性回归模型的参数和残差平方和\n",
    "    rss = Liner_model(data,features)\n",
    "    # 计算BIC值\n",
    "    n = len(data)*0.8\n",
    "    k = len(features)\n",
    "    \n",
    "    return n * np.log(rss / n) + k * np.log(n),rss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5db683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_features(data):\n",
    "    # 初始化特征集合为空\n",
    "    best_features = []\n",
    "    current_best_bic = []\n",
    "    current_best_feature = []\n",
    "    SSE = []\n",
    "    for a in range(data.shape[1] - 1):\n",
    "        # 初始化最小的BIC值为无穷大\n",
    "        Bics = []\n",
    "        cols = []\n",
    "        sses = []\n",
    "        # 遍历所有的特征\n",
    "        for i in range(data.shape[1] - 1):\n",
    "            col = current_best_feature.copy()\n",
    "            if i not in col:\n",
    "                col.append(i)\n",
    "                # 计算添加后的特征子集的BIC值\n",
    "                new_bic,sse = bic(data, col)\n",
    "                Bics.append(new_bic)\n",
    "                cols.append(col)\n",
    "                sses.append(sse)\n",
    "        current_best_feature = cols[np.array(sses).argmin()].copy()\n",
    "        current_best_bic.append(Bics[np.array(Bics).argmin()])\n",
    "        best_features.append(current_best_feature)\n",
    "        SSE.append(sses[np.array(Bics).argmin()])\n",
    "    return best_features,SSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acbf5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "def get_bis(features,path):\n",
    "    org_data = pd.read_excel(path).fillna(0)\n",
    "    columns = list(org_data.columns)\n",
    "    data_y = org_data[[columns[i] for i in features]]\n",
    "    model_1 = sm.OLS(org_data['Y'], sm.add_constant(data_y)).fit()\n",
    "    return model_1.bic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eebb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bic_choose_feature(best_features,path):\n",
    "    bics = []\n",
    "    for i in range(len(best_features)):\n",
    "        bic = get_bis(best_features[i],path)\n",
    "        bics.append(bic)\n",
    "    min_index = np.array(bics).argmin()\n",
    "    return best_features[min_index],bics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcfec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"高价值特征生成.xlsx\"\n",
    "data = pd.read_excel(path).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cdc162",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features,SSE = get_best_features(np.array(data))\n",
    "pd.DataFrame(best_features).to_excel(\"best_feature_candinate.xlsx\")\n",
    "bic_feature,bics = bic_choose_feature(best_features,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed7f502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bic_feature = best_features[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf99b40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sse_bics = pd.concat((pd.DataFrame(SSE),pd.DataFrame(bics)),axis=1)\n",
    "sse_bics.columns = [\"SSE\",\"BIC\"]\n",
    "sse_bics.to_excel(\"SSE_and_BIC.xlsx\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56bf844",
   "metadata": {},
   "outputs": [],
   "source": [
    "org_data = pd.read_excel(\"高价值特征生成.xlsx\").fillna(0)\n",
    "columns = list(org_data.columns)\n",
    "filter_data = org_data[[columns[i] for i in bic_feature]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb69b472",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = pd.concat((filter_data,org_data.iloc[:,-1:]),axis = 1)\n",
    "data3.to_excel(\"BIC筛选后特征数据集.xlsx\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc51b97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3.corr().to_excel(\"皮尔逊相关系数矩阵-特征筛选后.xlsx\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf677641",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168217f1",
   "metadata": {},
   "source": [
    "# 比较不同模型的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9c6e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7ff14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "def Liner_model(datasets):\n",
    "    skf = KFold(n_splits=5, shuffle=True,random_state=888)\n",
    "    intercepts =[]\n",
    "    coefficients = []\n",
    "    x = datasets[:,:-1]\n",
    "    y = datasets[:,-1:]\n",
    "    val_ys = []\n",
    "    pre_ys = []\n",
    "    r2s = []\n",
    "    for i, (trn_idx, val_idx) in enumerate(skf.split(x, y)):\n",
    "        trn_x, trn_y = x[trn_idx], y[trn_idx]\n",
    "        val_x, val_y = x[val_idx], y[val_idx]\n",
    "        # 创建并拟合线性回归模型\n",
    "        model = LinearRegression()\n",
    "        model.fit(trn_x, trn_y)\n",
    "        intercept = model.intercept_\n",
    "        # 获取参数\n",
    "        coefficient = model.coef_\n",
    "        intercepts.append(intercept)\n",
    "        coefficients.append(coefficient)\n",
    "        # 计算MAE，MSE和R2\n",
    "#         xgb_mae = metrics.mean_absolute_error(xgb_val, xgb_pre)\n",
    "#         xgb_mse = metrics.mean_squared_error(xgb_val, xgb_pre)\n",
    "        line_r2 = metrics.r2_score(val_y, model.predict(val_x))\n",
    "        r2s.append(line_r2)\n",
    "        pre_ys.append(model.predict(val_x))\n",
    "        val_ys.append(val_y)\n",
    "    return intercepts,coefficients,r2s,pre_ys,val_ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5aad9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"BIC筛选后特征数据集.xlsx\"\n",
    "data = pd.read_excel(path).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9c71f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d7af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"BIC筛选后特征数据集.xlsx\"\n",
    "data = pd.read_excel(path).fillna(0)\n",
    "data[\"Y\"] = data[\"Y\"] * scaler.scale_[-1] + scaler.mean_[-1] \n",
    "intercepts,coefficients,r2s,pre_ys,val_ys = Liner_model(np.array(data))\n",
    "\n",
    "# 导入机器学习包\n",
    "import sklearn.metrics as metrics\n",
    "line_val = [a for i in val_ys for a in i]\n",
    "line_pre = [a for i in pre_ys for a in i]\n",
    "\n",
    "# 计算MAE，MSE和R2\n",
    "line_mae = metrics.mean_absolute_error(line_val, line_pre)\n",
    "line_mse = metrics.mean_squared_error(line_val, line_pre)\n",
    "line_r2 = metrics.r2_score(line_val, line_pre)\n",
    "\n",
    "# 打印结果\n",
    "print(\"line_MAE:\", line_mae)\n",
    "print(\"line_MSE:\", line_mse)\n",
    "print(\"line_R2:\", line_r2)\n",
    "\n",
    "\n",
    "line_pre_data = pd.concat((pd.DataFrame(line_val),pd.DataFrame(line_pre)),axis = 1)\n",
    "line_pre_data.columns = [\"line_Actual\",\"line_Predict\"]\n",
    "line_pre_data.to_excel(\"line_pre_data.xlsx\",index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c041bc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d41fc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56761697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(f_path):\n",
    "    if f_path.endswith(\".csv\"):\n",
    "        data = pd.read_csv(f_path)\n",
    "    elif f_path.endswith(\".xls\") or f_path.endswith(\".xlsx\"):\n",
    "        data = pd.read_excel(f_path)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4f129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_train(params,data,Predict = False,Output = False):\n",
    "    dataset = np.array(data)\n",
    "    x = dataset[:,:-1]\n",
    "    y = dataset[:,-1:].reshape(-1)\n",
    "    n_splits = 5\n",
    "    skf = KFold(n_splits=n_splits, shuffle=True, random_state=2020)\n",
    "    min_mae = []\n",
    "    pre = 0\n",
    "    val_ys = []\n",
    "    pre_ys = []\n",
    "    for i, (trn_idx, val_idx) in enumerate(skf.split(x, y)):\n",
    "        trn_x, trn_y = x[trn_idx], y[trn_idx]\n",
    "        val_x, val_y = x[val_idx], y[val_idx]\n",
    "        rf_model = RandomForestRegressor(n_estimators=params[\"n_estimators\"],max_depth=params[\"max_depth\"])\n",
    "        rf_model.fit(trn_x,trn_y)\n",
    "        val_ys.append(val_y)\n",
    "        pre_ys.append(rf_model.predict(val_x))\n",
    "        if Predict is True:\n",
    "            pre += rf_model.predict(test)/n_splits\n",
    "        else:\n",
    "#                 min_r2.append(rf_model.score(val_x,val_y))\n",
    "            val_mae = abs(abs(rf_model.predict(val_x))-abs(val_y)).mean()\n",
    "            min_mae.append(val_mae)\n",
    "    if Predict is True:\n",
    "        return pre\n",
    "    elif Output is True:\n",
    "        return val_ys,pre_ys\n",
    "    else:     \n",
    "        return np.array(min_mae).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b678cecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_objective(trial,data):\n",
    "\n",
    "    params = {\n",
    "          'n_estimators': trial.suggest_int(\"n_estimators\", 10, 100,5),\n",
    "          'max_depth': trial.suggest_int(\"max_depth\", 1,10,1),\n",
    "          }\n",
    "\n",
    "    loss = RF_train(params,data)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d6ea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_optuna(n_trials, algo,optuna_objective,data):\n",
    "\n",
    "    #定义使用TPE或者GP\n",
    "    if algo == \"TPE\":\n",
    "        algo = optuna.samplers.TPESampler(n_startup_trials = 10, n_ei_candidates = 24)\n",
    "    elif algo == \"GP\":\n",
    "        from optuna.integration import SkoptSampler\n",
    "        import skopt\n",
    "        algo = SkoptSampler(skopt_kwargs={'base_estimator':'GP', #选择高斯过程\n",
    "                                          'n_initial_points':10, #初始观测点10个\n",
    "                                          'acq_func':'EI'} #选择的采集函数为EI，期望增量\n",
    "                           )\n",
    "\n",
    "    #实际优化过程，首先实例化优化器\n",
    "    study = optuna.create_study(sampler = algo #要使用的具体算法\n",
    "                                , direction=\"minimize\" #优化的方向，可以填写minimize或maximize\n",
    "                               )\n",
    "    #开始优化，n_trials为允许的最大迭代次数\n",
    "    #由于参数空间已经在目标函数中定义好，因此不需要输入参数空间\n",
    "    study.optimize(lambda trial: optuna_objective(trial, data) #目标函数\n",
    "                   , n_trials=n_trials #最大迭代次数（包括最初的观测值的）\n",
    "                   , show_progress_bar=True #要不要展示进度条呀？\n",
    "                  )\n",
    "\n",
    "    #可直接从优化好的对象study中调用优化的结果\n",
    "    #打印最佳参数与最佳损失值\n",
    "    print(\"\\n\",\"\\n\",\"best params: \", study.best_trial.params,\n",
    "          \"\\n\",\"\\n\",\"best score: \", study.best_trial.values,\n",
    "          \"\\n\")\n",
    "\n",
    "    return study.best_trial.params, study.best_trial.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ac8583",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./BIC筛选后特征数据集.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3959d8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13a6f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best_params,rf_best_mae = optimizer_optuna(n_trials = 100, algo=\"TPE\", optuna_objective=rf_objective, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc70e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ys,pre_ys= RF_train(params=rf_best_params,data=data,Output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ea7923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入机器学习包\n",
    "import sklearn.metrics as metrics\n",
    "rf_val = [a for i in val_ys for a in i]\n",
    "rf_pre = [a for i in pre_ys for a in i]\n",
    "\n",
    "rf_val = np.array(rf_val) * scaler.scale_[-1] + scaler.mean_[-1] \n",
    "rf_pre = np.array(rf_pre) * scaler.scale_[-1] + scaler.mean_[-1] \n",
    "# 计算MAE，MSE和R2\n",
    "rf_mae = metrics.mean_absolute_error(rf_val, rf_pre)\n",
    "rf_mse = metrics.mean_squared_error(rf_val, rf_pre)\n",
    "rf_r2 = metrics.r2_score(rf_val, rf_pre)\n",
    "\n",
    "# 打印结果\n",
    "print(\"RF_MAE:\", rf_mae)\n",
    "print(\"RF_MSE:\", rf_mse)\n",
    "print(\"RF_R2:\", rf_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c401c9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_pre_data = pd.concat((pd.DataFrame(rf_val),pd.DataFrame(rf_pre)),axis = 1)\n",
    "RF_pre_data.columns = [\"RF_Actual\",\"RF_Predict\"]\n",
    "RF_pre_data.to_excel(\"RF_pre_data.xlsx\",index =False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4515f44c",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dab39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13539ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_train(params, data, data2=None, Predict=False, Output=False):\n",
    "    dataset = np.array(data)\n",
    "    x = dataset[:, :-1]\n",
    "    y = dataset[:, -1:]\n",
    "    if Predict is True:\n",
    "        test_x = np.array(data2)\n",
    "    n_splits = 5\n",
    "    skf = KFold(n_splits=n_splits, shuffle=True, random_state=2020)\n",
    "    min_mae = []\n",
    "    pre = 0\n",
    "    \n",
    "    val_ys = []\n",
    "    pre_ys = []\n",
    "    evals_result = {}\n",
    "    for i, (trn_idx, val_idx) in enumerate(skf.split(x, y)):\n",
    "        trn_x, trn_y = x[trn_idx], y[trn_idx]\n",
    "        val_x, val_y = x[val_idx], y[val_idx]\n",
    "        dataTrain = xgb.DMatrix(trn_x, trn_y)\n",
    "        dataVal = xgb.DMatrix(val_x, val_y)\n",
    "        watchlist = [(dataVal, 'test'), (dataTrain, 'train')]\n",
    "        if Predict is True:\n",
    "            dataTest = xgb.DMatrix(test_x)\n",
    "            bst = xgb.train(params=params, dtrain=dataTrain, num_boost_round=5000, evals=watchlist,\n",
    "                            callbacks=[xgb.callback.EarlyStopping(50)])\n",
    "            pre += bst.predict(dataTest) / n_splits\n",
    "\n",
    "        \n",
    "        elif Output is True:\n",
    "            bst = xgb.train(params=params, dtrain=dataTrain, num_boost_round=5000, evals=watchlist,\n",
    "                            callbacks=[xgb.callback.EarlyStopping(50)],verbose_eval=False)\n",
    "            val_ys.append(val_y)\n",
    "            pre_ys.append(bst.predict(xgb.DMatrix(val_x)))\n",
    "            \n",
    "\n",
    "        else:\n",
    "            bst = xgb.train(params=params, dtrain=dataTrain, num_boost_round=5000, evals=watchlist,\n",
    "                            evals_result=evals_result, callbacks=[xgb.callback.EarlyStopping(50)],verbose_eval=False)\n",
    "            min_mae.append(min(list(evals_result[\"test\"].values())[0]))\n",
    "\n",
    "\n",
    "    if Predict is True:\n",
    "        return pre\n",
    "    elif Output is True:\n",
    "        return val_ys, pre_ys\n",
    "    else:\n",
    "        return np.array(min_mae).mean()\n",
    "    \n",
    "    \n",
    "    \n",
    "def xgb_objective(trial, data):\n",
    "    params = {\n",
    "        'subsample': trial.suggest_float(\"subsample\", 0.2, 0.8),\n",
    "        'eta': trial.suggest_float(\"eta\", 0.01, 0.1, step=0.01),\n",
    "        'max_depth': trial.suggest_int(\"max_depth\", 1, 5, 1),\n",
    "        'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.2, 0.8, step=0.1),\n",
    "        'gamma': trial.suggest_float(\"gamma\", 0, 1),\n",
    "        'eval_metric': \"mae\",\n",
    "        \"verbose\":0\n",
    "    }\n",
    "    loss = xgb_train(params, data)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cd5d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(params, data1, data2):\n",
    "    pre_reslust = xgb_train(params=params, data=data1, data2=data2, Predict=True)\n",
    "    return pre_reslust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadd684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./BIC筛选后特征数据集.xlsx\"\n",
    "data = read_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb115bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best_params, xgb_best_mae = optimizer_optuna(n_trials=100, algo=\"TPE\", optuna_objective=xgb_objective,\n",
    "                                               data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe7fe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ys, pre_ys = xgb_train(xgb_best_params, data, data2=None, Predict=False, Output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3670b7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入机器学习包\n",
    "import sklearn.metrics as metrics\n",
    "xgb_val = [a for i in val_ys for a in i]\n",
    "xgb_pre = [a for i in pre_ys for a in i]\n",
    "xgb_val = np.array(xgb_val) * scaler.scale_[-1] + scaler.mean_[-1] \n",
    "xgb_pre = np.array(xgb_pre) * scaler.scale_[-1] + scaler.mean_[-1] \n",
    "# 计算MAE，MSE和R2\n",
    "xgb_mae = metrics.mean_absolute_error(xgb_val, xgb_pre)\n",
    "xgb_mse = metrics.mean_squared_error(xgb_val, xgb_pre)\n",
    "xgb_r2 = metrics.r2_score(xgb_val, xgb_pre)\n",
    "\n",
    "# 打印结果\n",
    "print(\"XGB_MAE:\", xgb_mae)\n",
    "print(\"XGB_MSE:\", xgb_mse)\n",
    "print(\"XGB_R2:\", xgb_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8f31e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_pre_data = pd.concat((pd.DataFrame(xgb_val),pd.DataFrame(xgb_pre)),axis = 1)\n",
    "XGB_pre_data.columns = [\"XGB_Actual\",\"XGB_Predict\"]\n",
    "XGB_pre_data.to_excel(\"XGB_pre_data.xlsx\",index =False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c7585a",
   "metadata": {},
   "source": [
    "# 神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda6ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import pandas as pd\n",
    "import copy\n",
    "import numpy as np\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 搭建全连接神经网络回归\n",
    "class MLPregression(nn.Module):\n",
    "    def __init__(self, params, data):\n",
    "        super(MLPregression, self).__init__()\n",
    "        self.layber_number = params[\"layber_number\"]\n",
    "        self.unit = params[\"unit\"]\n",
    "        # 第一个隐含层\n",
    "        self.hidden1 = nn.Linear(in_features=data.shape[-1]-1, out_features=self.unit, bias=True)\n",
    "        # 第二个隐含层\n",
    "        self.hidden2 = nn.Linear(self.unit, self.unit)\n",
    "        # 第三个隐含层\n",
    "        self.hidden3 = nn.Linear(128, 256)\n",
    "        # 回归预测层\n",
    "        self.hidden5 = nn.Linear(self.unit, 64)\n",
    "        self.predict = nn.Linear(64, 1)\n",
    "        self.relu = nn.functional.relu\n",
    "        self.dropout = nn.Dropout(params[\"drop_out\"])\n",
    "\n",
    "    # 定义网络前向传播路径\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.hidden1(x))\n",
    "        for i in range(self.layber_number):\n",
    "            x = self.dropout(self.relu(self.hidden2(x)))\n",
    "        x = self.dropout(self.relu(self.hidden5(x)))\n",
    "        output = self.predict(x)\n",
    "        # 输出一个一维向量\n",
    "        return output[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce002ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastTensorDataLoader:\n",
    "    \"\"\"\n",
    "    A DataLoader-like object for a set of tensors that can be much faster than\n",
    "    TensorDataset + DataLoader because dataloader grabs individual indices of\n",
    "    the dataset and calls cat (slow).\n",
    "    Source: https://discuss.pytorch.org/t/dataloader-much-slower-than-manual-batching/27014/6\n",
    "    \"\"\"\n",
    "    def __init__(self, *tensors, batch_size=32, shuffle=False):\n",
    "        \"\"\"\n",
    "        Initialize a FastTensorDataLoader.\n",
    "        :param *tensors: tensors to store. Must have the same length @ dim 0.\n",
    "        :param batch_size: batch size to load.\n",
    "        :param shuffle: if True, shuffle the data *in-place* whenever an\n",
    "            iterator is created out of this object.\n",
    "        :returns: A FastTensorDataLoader.\n",
    "        \"\"\"\n",
    "        assert all(t.shape[0] == tensors[0].shape[0] for t in tensors)\n",
    "        self.tensors = tensors\n",
    "\n",
    "        self.dataset_len = self.tensors[0].shape[0]\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        # Calculate # batches\n",
    "        n_batches, remainder = divmod(self.dataset_len, self.batch_size)\n",
    "        if remainder > 0:\n",
    "            n_batches += 1\n",
    "        self.n_batches = n_batches\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            r = torch.randperm(self.dataset_len)\n",
    "            self.tensors = [t[r] for t in self.tensors]\n",
    "        self.i = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.i >= self.dataset_len:\n",
    "            raise StopIteration\n",
    "        batch = tuple(t[self.i:self.i+self.batch_size] for t in self.tensors)\n",
    "        self.i += self.batch_size\n",
    "        return batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4682225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(params, model, dataloaders, criterion, optimizer, num_epochs, i, savebest=False,Output=False):\n",
    "    since = time.time()\n",
    "    best_loss = 10000000\n",
    "    model.to(device)\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # 训练和验证\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # 训练\n",
    "            else:\n",
    "                model.eval()  # 验证\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # 把数据都取个遍\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # 清零\n",
    "                optimizer.zero_grad()\n",
    "                # 只有训练的时候计算和更新梯度\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    #                     print(outputs.shape, labels.shape)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    #                     print(\"loss为：\",loss)\n",
    "                    # 训练阶段更新权重\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                #                 计算损失\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "            epoch_loss = running_loss / dataloaders[phase].dataset_len\n",
    "            #             print(epoch_loss)\n",
    "            time_elapsed = time.time() - since\n",
    "            #             print('Time elapsed {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "            #             print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # 得到最好那次的模型\n",
    "            if phase == 'valid' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "#                 print(best_loss)\n",
    "#                 print(outputs[0],labels[0])\n",
    "                if savebest is True:\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    state = {\n",
    "                        'state_dict': model.state_dict(),  # 字典里key就是各层的名字，值就是训练好的权重\n",
    "                        'best_loss': best_loss,\n",
    "                        'optimizer': optimizer.state_dict(),  # 优化器的状态信息\n",
    "                    }\n",
    "                    filename = './NN_best' + str(i) + '.pth'\n",
    "                    torch.save(state, filename)\n",
    "                    inputs_list = []\n",
    "                    labels_list = []\n",
    "                    for inputs, labels in dataloaders[phase]:\n",
    "                        inputs = inputs.to(device)\n",
    "                        labels = labels.to(device)\n",
    "                        inputs_list.append(inputs)\n",
    "                        labels_list.append(labels)\n",
    "                    final_inputs = torch.cat(inputs_list, dim=0)\n",
    "                    final_labels = torch.cat(labels_list, dim=0)\n",
    "                    pre_y = model(final_inputs)\n",
    "                    val_y = final_labels\n",
    "\n",
    "                    \n",
    "            if phase == 'valid':\n",
    "                valid_losses.append(epoch_loss)\n",
    "            #                 scheduler.step(epoch_loss)#学习率衰减\n",
    "            if phase == 'train':\n",
    "                train_losses.append(epoch_loss)\n",
    "\n",
    "    if savebest is True:\n",
    "        return train_losses, valid_losses, best_loss, pre_y,val_y\n",
    "    else:\n",
    "        return best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd93199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cross_train(params, data, savebest=False):\n",
    "    x = data[:, :-1]\n",
    "    y = data[:, -1:]\n",
    "    loss = []\n",
    "    skf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    val_ys = []\n",
    "    pre_ys = []\n",
    "    for i, (trn_idx, val_idx) in enumerate(skf.split(x, y)):\n",
    "        trn_x, trn_y = x[trn_idx], y[trn_idx]\n",
    "        val_x, val_y = x[val_idx], y[val_idx]\n",
    "#         print(f\"val is {len(val_y)}\")\n",
    "        # 将数据集转为张量\n",
    "        X_train_t = torch.from_numpy(trn_x.astype(np.float32))\n",
    "        y_train_t = torch.from_numpy(trn_y.squeeze().astype(np.float32))\n",
    "        X_valid_t = torch.from_numpy(val_x.astype(np.float32))\n",
    "        y_valid_t = torch.from_numpy(val_y.squeeze().astype(np.float32))\n",
    "\n",
    "        # 将训练数据处理为数据加载器\n",
    "        #         train_data = Data.TensorDataset(X_train_t, y_train_t)\n",
    "        #         valid_data = Data.TensorDataset(X_valid_t, y_valid_t)\n",
    "\n",
    "        #         train_loader = Data.DataLoader(dataset = train_data, batch_size = params['batch_size'],\n",
    "        #                                        shuffle = True, num_workers = 1)\n",
    "        #         val_loader = Data.DataLoader(dataset = valid_data, batch_size = params['batch_size'],\n",
    "        #                                        shuffle = True, num_workers = 1)\n",
    "        train_loader = FastTensorDataLoader(X_train_t, y_train_t, batch_size=params['batch_size'],\n",
    "                                            shuffle=True)\n",
    "        val_loader = FastTensorDataLoader(X_valid_t, y_valid_t, batch_size=params['batch_size'],\n",
    "                                          shuffle=True)\n",
    "        # 实例化\n",
    "        model = MLPregression(params, data)\n",
    "        # 损失函数\n",
    "        criterion = torch.nn.L1Loss()\n",
    "        # 优化器设置\n",
    "        optimizer_ft = optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "        # scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)#学习率每7个epoch衰减成原来的1/1\n",
    "        # 数据加载器\n",
    "        dataloaders = {'train': train_loader, 'valid': val_loader}\n",
    "        \n",
    "        if savebest is True:\n",
    "            train_loss, valid_loss, best_loss, pre_y,act_y = train_model(params, model, dataloaders, criterion, optimizer_ft,\n",
    "                                                            num_epochs=100, i=i, savebest=True)\n",
    "            train_losses.append(train_loss)\n",
    "            valid_losses.append(valid_loss)\n",
    "            loss.append(best_loss)\n",
    "            val_ys.append(act_y)\n",
    "            pre_ys.append(pre_y)\n",
    "        else:\n",
    "            best_loss = train_model(params, model, dataloaders, criterion, optimizer_ft, num_epochs=100, i=i)\n",
    "            loss.append(best_loss)\n",
    "\n",
    "    if savebest is True:\n",
    "        return train_losses, valid_losses, loss,val_ys,pre_ys\n",
    "    else:\n",
    "        return np.array(loss).mean()\n",
    "\n",
    "\n",
    "def model_objective(trial, data):\n",
    "    params = {\n",
    "          'lr': trial.suggest_float(\"lr\", 1e-5, 1e-2),\n",
    "          'batch_size': trial.suggest_int(\"batch_size\", 16,128,16),\n",
    "          'drop_out':trial.suggest_float(\"drop_out\", 0, 0.2),\n",
    "          'unit': trial.suggest_int(\"unit\", 16, 128,16),\n",
    "          'layber_number':trial.suggest_int(\"layber_number\", 1, 16,1)\n",
    "          }\n",
    "    loss = model_cross_train(params, data)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5942a4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(f_path, seed=42): \n",
    "    df_all = pd.read_excel(f_path)\n",
    "    #create a min max processing object\n",
    "    composition = df_all.iloc[:,:-1]\n",
    "    scaler = preprocessing.StandardScaler().fit(composition)\n",
    "    normalized_composition = scaler.transform(composition)\n",
    "    \n",
    "    return normalized_composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56257b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./BIC筛选后特征数据集.xlsx\"\n",
    "# file_path = \"./原始数据2.xlsx\"\n",
    "test_x = data_generator(file_path, seed=42)\n",
    "label = np.array(pd.read_excel(file_path)[\"Y\"]).reshape(-1,1)\n",
    "data =np.hstack((test_x,label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6729c15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"./BIC筛选后特征数据集.xlsx\"\n",
    "# data = pd.read_excel(path).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f85b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a562bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_best_params, nn_best_mae = optimizer_optuna(n_trials=100, algo=\"TPE\", optuna_objective=model_objective,\n",
    "                                               data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac0e863",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, valid_losses, loss,val_ys,pre_ys = model_cross_train(nn_best_params, data, savebest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33fd4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.L1Loss()\n",
    "criterion(pre_ys[1].cpu(),val_ys[1].cpu())\n",
    "# 导入机器学习包\n",
    "import sklearn.metrics as metrics\n",
    "nn_val = [a for i in val_ys for a in i.cpu().data.numpy()]\n",
    "nn_pre = [a for i in pre_ys for a in i.cpu().data.numpy()]\n",
    "\n",
    "nn_val = np.array(nn_val) * scaler.scale_[-1] + scaler.mean_[-1] \n",
    "nn_pre = np.array(nn_pre) * scaler.scale_[-1] + scaler.mean_[-1] \n",
    "# 计算MAE，MSE和R2\n",
    "nn_mae = metrics.mean_absolute_error(nn_val, nn_pre)\n",
    "nn_mse = metrics.mean_squared_error(nn_val, nn_pre)\n",
    "nn_r2 = metrics.r2_score(nn_val, nn_pre)\n",
    "\n",
    "# 打印结果\n",
    "print(\"NN_MAE:\", nn_mae)\n",
    "print(\"NN_MSE:\", nn_mse)\n",
    "print(\"NN_R2:\", nn_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999f4cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_pre_data = pd.concat((pd.DataFrame(nn_val),pd.DataFrame(nn_pre)),axis = 1)\n",
    "NN_pre_data.columns = [\"NN_Actual\",\"NN_Predict\"]\n",
    "NN_pre_data.to_excel(\"NN_pre_data.xlsx\",index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74ea9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nn_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8078f376",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:matterai]",
   "language": "python",
   "name": "conda-env-matterai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
